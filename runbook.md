# ðŸ“• Operational Runbook: Blue/Green Observability (Stage 3)

This runbook outlines the required actions for operators when specific alerts are generated by the `alert_watcher` service.

## 1. Environment Variables / Maintenance

The alert watcher is controlled by the following environment variables defined in `.env`:

| Variable | Description | Default | Operator Action |
| :--- | :--- | :--- | :--- |
| `SLACK_WEBHOOK_URL` | Destination for all alerts. | N/A | Verify URL is functional if alerts fail to send. |
| `ACTIVE_POOL` | The pool expected to be serving traffic (`blue` or `green`). | `blue` | **Action:** This is the control switch for manual toggles. |
| `MAINTENANCE_MODE` | Set to `true` to suppress all alerts during planned maintenance. | `false` | **Action:** Set to `true` before planned deployment/chaos testing. |

## 2. Alert Definitions and Actions

| Alert Title | Trigger Condition | Operator Action | Resolution |
| :--- | :--- | :--- | :--- |
| **Failover Detected! ðŸ”„** | The `X-App-Pool` header in the Nginx logs flips from the expected primary pool to the backup pool. | 1. **CHECK** the health of the *old* primary container (`app_blue` if failure was blueâ†’green) via `docker-compose logs app_blue`. Look for crash logs or high error rates. 2. **CONFIRM** the backup pool is stable by checking application logs (`docker-compose logs app_green`). | If the primary cannot be fixed quickly, consider a **manual flip** of the `ACTIVE_POOL` variable to formalize the new primary, followed by a restart of Nginx. |
| **Error Rate High! ðŸ”¥** | 5xx errors from the upstream pool exceed the `ERROR_RATE_THRESHOLD` (e.g., >2%) over the `WINDOW_SIZE` (e.g., last 200 requests). | 1. **INSPECT** Nginx logs (`docker-compose logs nginx`) for the upstream server address causing the failures. 2. **ISOLATE** the failing container and inspect its application logs for exceptions (`docker-compose logs app_X`). 3. **Action:** If the failure is systemic and affecting users, immediately perform a **manual flip** of the `ACTIVE_POOL` and recreate the Nginx container to use the backup as the formal primary. | Followed by a **Recovery Alert** when the rate drops below the threshold. |
| **Error Rate Recovered! ðŸŸ¢** | The 5xx error rate has dropped below the `ERROR_RATE_THRESHOLD`. | **INFORMATIONAL ONLY.** This confirms the previous alert has cleared, either due to the Nginx failover to the healthy backup or because the primary service has self-healed. **Action:** Review the healthcheck status of the container that was previously failing. | Review healthcheck status; no immediate action required. |

## 3. Chaos Drills (Testing)

**To generate a Failover Alert (Blue â†’ Green):**
1. Ensure `ACTIVE_POOL=blue` in `.env`.
2. Inject chaos on the primary: `curl -X POST http://localhost:8081/chaos/start?mode=error`
3. Send a few requests to Nginx (`http://localhost:8080/version`). Nginx will fail over.
4. An alert should post to Slack.

**To generate a High Error Rate Alert (No Failover):**
1. Set `ERROR_RATE_THRESHOLD=5` (5%) in `.env`.
2. Inject chaos: `curl -X POST http://localhost:8081/chaos/start?mode=error`
3. Send **many** requests (more than `WINDOW_SIZE` requests) to Nginx (`http://localhost:8080/version`).
    * *Note: Since the Nginx failover is fast, this test is tricky.* You can modify the Nginx `max_fails` to 10 to intentionally keep the failing pool active long enough to fill the window.
4. An alert should post to Slack before Nginx marks the pool as completely down.